# CSE417 Data Mining Fall 2018 @WUSTL

## Neutral Network (Autoencoder decimal to binary) built in Python using Keras

#### The input layers and the output layers all have 16 nodes. Sigmod function (i.e., f(x) = 1/(1+e^(-x))) is used for perceptron activation 
###### 1.	A 3-layer NN (with layers for input, hidden and output). The hidden layer has 5 perceptrons. 
###### 2.	A 3-layer NN. The hidden layer has 4 perceptrons.
###### 3.	A 3-layer NN. The hidden layer has 3 perceptrons. 
###### 4.	A 5-layer NN. The 1st, 2nd, and 3rd hidden layers have 8, 4, and 8 perceptrons, respectively. 
###### 5.	Repeat 4) above, but this time we use the Rectified Linear Unit (ReLU), i.e., f(u) = max(0, u), for all perceptron activation. 
